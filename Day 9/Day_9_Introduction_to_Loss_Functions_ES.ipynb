{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_9_Introduction_to_Loss_Functions_ES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esha-sidhu/FutureMakers2022/blob/main/Day%209/Day_9_Introduction_to_Loss_Functions_ES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9932a638-e3fa-4d2c-8e83-d8a9f056ef9f"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c576a265-64b2-4d50-cdd6-447374383585"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 3s 16ms/step - loss: 583.5986 - mse: 583.5986 - val_loss: 485.8010 - val_mse: 485.8010\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 564.2084 - mse: 564.2084 - val_loss: 466.5621 - val_mse: 466.5621\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 538.8801 - mse: 538.8801 - val_loss: 439.0049 - val_mse: 439.0049\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 502.0754 - mse: 502.0754 - val_loss: 399.7302 - val_mse: 399.7302\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 452.6962 - mse: 452.6962 - val_loss: 345.6853 - val_mse: 345.6853\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 383.8928 - mse: 383.8928 - val_loss: 276.5455 - val_mse: 276.5455\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 298.9205 - mse: 298.9205 - val_loss: 195.2328 - val_mse: 195.2328\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 205.7178 - mse: 205.7178 - val_loss: 116.1331 - val_mse: 116.1331\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 121.8893 - mse: 121.8893 - val_loss: 61.0209 - val_mse: 61.0209\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 70.0061 - mse: 70.0061 - val_loss: 42.0511 - val_mse: 42.0511\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 50.8530 - mse: 50.8530 - val_loss: 37.4235 - val_mse: 37.4235\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 41.3977 - mse: 41.3977 - val_loss: 32.4081 - val_mse: 32.4081\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 34.7454 - mse: 34.7454 - val_loss: 28.5785 - val_mse: 28.5785\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.1706 - mse: 31.1706 - val_loss: 26.9412 - val_mse: 26.9412\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.5902 - mse: 28.5902 - val_loss: 26.1532 - val_mse: 26.1532\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27.1313 - mse: 27.1313 - val_loss: 26.0932 - val_mse: 26.0932\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26.0412 - mse: 26.0412 - val_loss: 25.3589 - val_mse: 25.3589\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25.1832 - mse: 25.1832 - val_loss: 24.3692 - val_mse: 24.3692\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 24.4526 - mse: 24.4526 - val_loss: 23.8351 - val_mse: 23.8351\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23.7859 - mse: 23.7859 - val_loss: 23.3005 - val_mse: 23.3005\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23.2154 - mse: 23.2154 - val_loss: 22.9632 - val_mse: 22.9632\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.6729 - mse: 22.6729 - val_loss: 22.5438 - val_mse: 22.5438\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.1317 - mse: 22.1317 - val_loss: 21.7821 - val_mse: 21.7821\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.7100 - mse: 21.7100 - val_loss: 21.2058 - val_mse: 21.2058\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.2489 - mse: 21.2489 - val_loss: 20.7368 - val_mse: 20.7368\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.7718 - mse: 20.7718 - val_loss: 20.1458 - val_mse: 20.1458\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.2592 - mse: 20.2592 - val_loss: 20.2704 - val_mse: 20.2704\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.1177 - mse: 20.1177 - val_loss: 19.9010 - val_mse: 19.9010\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.6226 - mse: 19.6226 - val_loss: 19.3049 - val_mse: 19.3049\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.3121 - mse: 19.3121 - val_loss: 19.3500 - val_mse: 19.3500\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.8230 - mse: 18.8230 - val_loss: 18.3503 - val_mse: 18.3503\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.3926 - mse: 18.3926 - val_loss: 18.1389 - val_mse: 18.1389\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.1248 - mse: 18.1248 - val_loss: 17.9394 - val_mse: 17.9394\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.7894 - mse: 17.7894 - val_loss: 16.9286 - val_mse: 16.9286\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.4860 - mse: 17.4860 - val_loss: 16.7623 - val_mse: 16.7623\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.1351 - mse: 17.1351 - val_loss: 16.2359 - val_mse: 16.2359\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.7266 - mse: 16.7266 - val_loss: 16.1181 - val_mse: 16.1181\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.5935 - mse: 16.5935 - val_loss: 15.9841 - val_mse: 15.9841\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16.1187 - mse: 16.1187 - val_loss: 15.1003 - val_mse: 15.1003\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16.0265 - mse: 16.0265 - val_loss: 14.7959 - val_mse: 14.7959\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.7682 - mse: 15.7682 - val_loss: 14.8415 - val_mse: 14.8415\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.3699 - mse: 15.3699 - val_loss: 14.0742 - val_mse: 14.0742\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15.0629 - mse: 15.0629 - val_loss: 13.4549 - val_mse: 13.4549\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.7913 - mse: 14.7913 - val_loss: 13.2992 - val_mse: 13.2992\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.5880 - mse: 14.5880 - val_loss: 13.0932 - val_mse: 13.0932\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.3392 - mse: 14.3392 - val_loss: 12.4646 - val_mse: 12.4646\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.0030 - mse: 14.0030 - val_loss: 12.4217 - val_mse: 12.4217\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7625 - mse: 13.7625 - val_loss: 11.9727 - val_mse: 11.9727\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5168 - mse: 13.5168 - val_loss: 11.6846 - val_mse: 11.6846\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.3994 - mse: 13.3994 - val_loss: 11.2487 - val_mse: 11.2487\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.1388 - mse: 13.1388 - val_loss: 10.9063 - val_mse: 10.9063\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.9690 - mse: 12.9690 - val_loss: 10.9782 - val_mse: 10.9782\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.5991 - mse: 12.5991 - val_loss: 10.3806 - val_mse: 10.3806\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.4280 - mse: 12.4280 - val_loss: 10.1916 - val_mse: 10.1916\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.2109 - mse: 12.2109 - val_loss: 9.8423 - val_mse: 9.8423\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.0562 - mse: 12.0562 - val_loss: 9.7809 - val_mse: 9.7809\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.8495 - mse: 11.8495 - val_loss: 9.4269 - val_mse: 9.4269\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7711 - mse: 11.7711 - val_loss: 9.4891 - val_mse: 9.4891\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.6194 - mse: 11.6194 - val_loss: 9.3504 - val_mse: 9.3504\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3698 - mse: 11.3698 - val_loss: 9.2013 - val_mse: 9.2013\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2370 - mse: 11.2370 - val_loss: 9.1534 - val_mse: 9.1534\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.0657 - mse: 11.0657 - val_loss: 8.8709 - val_mse: 8.8709\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9342 - mse: 10.9342 - val_loss: 8.6538 - val_mse: 8.6538\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.8899 - mse: 10.8899 - val_loss: 8.6283 - val_mse: 8.6283\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6525 - mse: 10.6525 - val_loss: 8.5877 - val_mse: 8.5877\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7294 - mse: 10.7294 - val_loss: 8.2650 - val_mse: 8.2650\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4532 - mse: 10.4532 - val_loss: 8.0625 - val_mse: 8.0625\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3616 - mse: 10.3616 - val_loss: 8.0925 - val_mse: 8.0925\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2579 - mse: 10.2579 - val_loss: 8.0533 - val_mse: 8.0533\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2328 - mse: 10.2328 - val_loss: 8.0194 - val_mse: 8.0194\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0858 - mse: 10.0858 - val_loss: 8.0199 - val_mse: 8.0199\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2003 - mse: 10.2003 - val_loss: 7.7580 - val_mse: 7.7580\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9864 - mse: 9.9864 - val_loss: 7.8165 - val_mse: 7.8165\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8214 - mse: 9.8214 - val_loss: 7.5913 - val_mse: 7.5913\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7997 - mse: 9.7997 - val_loss: 7.6809 - val_mse: 7.6809\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7524 - mse: 9.7524 - val_loss: 7.7703 - val_mse: 7.7703\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6516 - mse: 9.6516 - val_loss: 7.4172 - val_mse: 7.4172\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5401 - mse: 9.5401 - val_loss: 7.5471 - val_mse: 7.5471\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6523 - mse: 9.6523 - val_loss: 7.7715 - val_mse: 7.7715\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2812 - mse: 9.2812 - val_loss: 7.3659 - val_mse: 7.3659\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4857 - mse: 9.4857 - val_loss: 7.1702 - val_mse: 7.1702\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.2130 - mse: 9.2130 - val_loss: 7.6658 - val_mse: 7.6658\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2053 - mse: 9.2053 - val_loss: 7.1326 - val_mse: 7.1326\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0587 - mse: 9.0587 - val_loss: 7.3034 - val_mse: 7.3034\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0029 - mse: 9.0029 - val_loss: 7.1670 - val_mse: 7.1670\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9407 - mse: 8.9407 - val_loss: 7.0965 - val_mse: 7.0965\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8294 - mse: 8.8294 - val_loss: 7.1951 - val_mse: 7.1951\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7939 - mse: 8.7939 - val_loss: 7.0920 - val_mse: 7.0920\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7015 - mse: 8.7015 - val_loss: 7.0523 - val_mse: 7.0523\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7706 - mse: 8.7706 - val_loss: 6.8444 - val_mse: 6.8444\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6617 - mse: 8.6617 - val_loss: 7.0630 - val_mse: 7.0630\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5961 - mse: 8.5961 - val_loss: 6.9680 - val_mse: 6.9680\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4877 - mse: 8.4877 - val_loss: 7.0247 - val_mse: 7.0247\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4909 - mse: 8.4909 - val_loss: 6.9064 - val_mse: 6.9064\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3585 - mse: 8.3585 - val_loss: 6.9966 - val_mse: 6.9966\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3429 - mse: 8.3429 - val_loss: 7.1100 - val_mse: 7.1100\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4562 - mse: 8.4562 - val_loss: 6.9658 - val_mse: 6.9658\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2247 - mse: 8.2247 - val_loss: 7.1005 - val_mse: 7.1005\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1518 - mse: 8.1518 - val_loss: 6.8593 - val_mse: 6.8593\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1850 - mse: 8.1850 - val_loss: 6.8157 - val_mse: 6.8157\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3407 - mse: 8.3407 - val_loss: 7.2346 - val_mse: 7.2346\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1691 - mse: 8.1691 - val_loss: 6.8939 - val_mse: 6.8939\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9859 - mse: 7.9859 - val_loss: 6.8499 - val_mse: 6.8499\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8751 - mse: 7.8751 - val_loss: 6.9156 - val_mse: 6.9156\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8703 - mse: 7.8703 - val_loss: 6.8730 - val_mse: 6.8730\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7984 - mse: 7.7984 - val_loss: 6.8300 - val_mse: 6.8300\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8282 - mse: 7.8282 - val_loss: 6.7852 - val_mse: 6.7852\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7897 - mse: 7.7897 - val_loss: 6.8036 - val_mse: 6.8036\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6938 - mse: 7.6938 - val_loss: 6.9380 - val_mse: 6.9380\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7021 - mse: 7.7021 - val_loss: 6.7466 - val_mse: 6.7466\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6013 - mse: 7.6013 - val_loss: 6.6805 - val_mse: 6.6805\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5788 - mse: 7.5788 - val_loss: 6.7704 - val_mse: 6.7704\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4955 - mse: 7.4955 - val_loss: 6.6955 - val_mse: 6.6955\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4998 - mse: 7.4998 - val_loss: 6.8919 - val_mse: 6.8919\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4962 - mse: 7.4962 - val_loss: 6.6624 - val_mse: 6.6624\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4468 - mse: 7.4468 - val_loss: 6.7136 - val_mse: 6.7136\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3350 - mse: 7.3350 - val_loss: 6.8390 - val_mse: 6.8390\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3624 - mse: 7.3624 - val_loss: 6.7106 - val_mse: 6.7106\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3670 - mse: 7.3670 - val_loss: 6.6053 - val_mse: 6.6053\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3237 - mse: 7.3237 - val_loss: 6.7498 - val_mse: 6.7498\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3125 - mse: 7.3125 - val_loss: 6.6874 - val_mse: 6.6874\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3171 - mse: 7.3171 - val_loss: 6.7392 - val_mse: 6.7392\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2018 - mse: 7.2018 - val_loss: 6.7323 - val_mse: 6.7323\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1082 - mse: 7.1082 - val_loss: 6.7411 - val_mse: 6.7411\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1825 - mse: 7.1825 - val_loss: 6.7912 - val_mse: 6.7912\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0887 - mse: 7.0887 - val_loss: 6.7277 - val_mse: 6.7277\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0812 - mse: 7.0812 - val_loss: 6.8169 - val_mse: 6.8169\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3541 - mse: 7.3541 - val_loss: 6.7679 - val_mse: 6.7679\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9924 - mse: 6.9924 - val_loss: 6.7739 - val_mse: 6.7739\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9592 - mse: 6.9592 - val_loss: 6.7932 - val_mse: 6.7932\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0572 - mse: 7.0572 - val_loss: 6.9078 - val_mse: 6.9078\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9068 - mse: 6.9068 - val_loss: 6.7615 - val_mse: 6.7615\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9103 - mse: 6.9103 - val_loss: 6.7994 - val_mse: 6.7994\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0327 - mse: 7.0327 - val_loss: 6.8498 - val_mse: 6.8498\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9462 - mse: 6.9462 - val_loss: 6.6537 - val_mse: 6.6537\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9181 - mse: 6.9181 - val_loss: 6.9110 - val_mse: 6.9110\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8139 - mse: 6.8139 - val_loss: 6.7439 - val_mse: 6.7439\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7778 - mse: 6.7778 - val_loss: 6.8827 - val_mse: 6.8827\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8011 - mse: 6.8011 - val_loss: 6.9356 - val_mse: 6.9356\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6927 - mse: 6.6927 - val_loss: 6.8949 - val_mse: 6.8949\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7108 - mse: 6.7108 - val_loss: 6.8490 - val_mse: 6.8490\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6045 - mse: 6.6045 - val_loss: 6.8404 - val_mse: 6.8404\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7041 - mse: 6.7041 - val_loss: 6.9588 - val_mse: 6.9588\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6985 - mse: 6.6985 - val_loss: 6.8211 - val_mse: 6.8211\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5800 - mse: 6.5800 - val_loss: 6.7531 - val_mse: 6.7531\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5720 - mse: 6.5720 - val_loss: 6.8058 - val_mse: 6.8058\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5747 - mse: 6.5747 - val_loss: 6.8223 - val_mse: 6.8223\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5801 - mse: 6.5801 - val_loss: 6.8713 - val_mse: 6.8713\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4939 - mse: 6.4939 - val_loss: 6.8116 - val_mse: 6.8116\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4842 - mse: 6.4842 - val_loss: 6.9805 - val_mse: 6.9805\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5781 - mse: 6.5781 - val_loss: 7.0429 - val_mse: 7.0429\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4213 - mse: 6.4213 - val_loss: 7.0584 - val_mse: 7.0584\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4072 - mse: 6.4072 - val_loss: 6.8056 - val_mse: 6.8056\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3958 - mse: 6.3958 - val_loss: 6.8884 - val_mse: 6.8884\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4377 - mse: 6.4377 - val_loss: 6.8794 - val_mse: 6.8794\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3732 - mse: 6.3732 - val_loss: 6.9572 - val_mse: 6.9572\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3539 - mse: 6.3539 - val_loss: 7.1162 - val_mse: 7.1162\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3078 - mse: 6.3078 - val_loss: 6.8916 - val_mse: 6.8916\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3741 - mse: 6.3741 - val_loss: 7.0490 - val_mse: 7.0490\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2558 - mse: 6.2558 - val_loss: 7.0486 - val_mse: 7.0486\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3781 - mse: 6.3781 - val_loss: 7.0854 - val_mse: 7.0854\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2413 - mse: 6.2413 - val_loss: 6.9438 - val_mse: 6.9438\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2360 - mse: 6.2360 - val_loss: 7.1473 - val_mse: 7.1473\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2374 - mse: 6.2374 - val_loss: 7.0399 - val_mse: 7.0399\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1825 - mse: 6.1825 - val_loss: 7.0431 - val_mse: 7.0431\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1564 - mse: 6.1564 - val_loss: 7.0777 - val_mse: 7.0777\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2715 - mse: 6.2715 - val_loss: 7.0739 - val_mse: 7.0739\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0923 - mse: 6.0923 - val_loss: 7.1243 - val_mse: 7.1243\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2748 - mse: 6.2748 - val_loss: 7.3206 - val_mse: 7.3206\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0703 - mse: 6.0703 - val_loss: 7.0240 - val_mse: 7.0240\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0622 - mse: 6.0622 - val_loss: 7.1585 - val_mse: 7.1585\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0715 - mse: 6.0715 - val_loss: 7.2465 - val_mse: 7.2465\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0171 - mse: 6.0171 - val_loss: 6.9994 - val_mse: 6.9994\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0182 - mse: 6.0182 - val_loss: 7.1337 - val_mse: 7.1337\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0795 - mse: 6.0795 - val_loss: 7.2635 - val_mse: 7.2635\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3212 - mse: 6.3212 - val_loss: 7.1378 - val_mse: 7.1378\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2484 - mse: 6.2484 - val_loss: 7.4680 - val_mse: 7.4680\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0233 - mse: 6.0233 - val_loss: 6.9458 - val_mse: 6.9458\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8745 - mse: 5.8745 - val_loss: 7.1419 - val_mse: 7.1419\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8891 - mse: 5.8891 - val_loss: 7.1830 - val_mse: 7.1830\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8243 - mse: 5.8243 - val_loss: 7.1852 - val_mse: 7.1852\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8732 - mse: 5.8732 - val_loss: 7.3970 - val_mse: 7.3970\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7992 - mse: 5.7992 - val_loss: 7.2085 - val_mse: 7.2085\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8006 - mse: 5.8006 - val_loss: 7.2852 - val_mse: 7.2852\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8546 - mse: 5.8546 - val_loss: 7.3411 - val_mse: 7.3411\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8337 - mse: 5.8337 - val_loss: 7.2107 - val_mse: 7.2107\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8009 - mse: 5.8009 - val_loss: 7.3904 - val_mse: 7.3904\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7635 - mse: 5.7635 - val_loss: 7.1778 - val_mse: 7.1778\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8631 - mse: 5.8631 - val_loss: 7.3913 - val_mse: 7.3913\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7091 - mse: 5.7091 - val_loss: 7.3182 - val_mse: 7.3182\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9706 - mse: 5.9706 - val_loss: 7.4010 - val_mse: 7.4010\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6953 - mse: 5.6953 - val_loss: 7.4795 - val_mse: 7.4795\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8887 - mse: 5.8887 - val_loss: 7.2677 - val_mse: 7.2677\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7248 - mse: 5.7248 - val_loss: 7.2908 - val_mse: 7.2908\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9266 - mse: 5.9266 - val_loss: 7.2927 - val_mse: 7.2927\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7521 - mse: 5.7521 - val_loss: 7.3617 - val_mse: 7.3617\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6445 - mse: 5.6445 - val_loss: 7.5104 - val_mse: 7.5104\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5569 - mse: 5.5569 - val_loss: 7.2284 - val_mse: 7.2284\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5261 - mse: 5.5261 - val_loss: 7.3033 - val_mse: 7.3033\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6189 - mse: 5.6189 - val_loss: 7.3177 - val_mse: 7.3177\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6900 - mse: 5.6900 - val_loss: 7.3131 - val_mse: 7.3131\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5267 - mse: 5.5267 - val_loss: 7.3288 - val_mse: 7.3288\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5284 - mse: 5.5284 - val_loss: 7.3279 - val_mse: 7.3279\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4857 - mse: 5.4857 - val_loss: 7.2824 - val_mse: 7.2824\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5455 - mse: 5.5455 - val_loss: 7.3981 - val_mse: 7.3981\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5467 - mse: 5.5467 - val_loss: 7.2587 - val_mse: 7.2587\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8754 - mse: 5.8754 - val_loss: 7.5109 - val_mse: 7.5109\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4197 - mse: 5.4197 - val_loss: 7.2704 - val_mse: 7.2704\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4065 - mse: 5.4065 - val_loss: 7.3563 - val_mse: 7.3563\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3687 - mse: 5.3687 - val_loss: 7.3595 - val_mse: 7.3595\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3367 - mse: 5.3367 - val_loss: 7.5022 - val_mse: 7.5022\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4892 - mse: 5.4892 - val_loss: 7.4558 - val_mse: 7.4558\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4454 - mse: 5.4454 - val_loss: 7.5425 - val_mse: 7.5425\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3337 - mse: 5.3337 - val_loss: 7.3591 - val_mse: 7.3591\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3264 - mse: 5.3264 - val_loss: 7.4582 - val_mse: 7.4582\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3792 - mse: 5.3792 - val_loss: 7.4156 - val_mse: 7.4156\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2777 - mse: 5.2777 - val_loss: 7.5130 - val_mse: 7.5130\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2935 - mse: 5.2935 - val_loss: 7.2666 - val_mse: 7.2666\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3003 - mse: 5.3003 - val_loss: 7.4879 - val_mse: 7.4879\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2202 - mse: 5.2202 - val_loss: 7.2629 - val_mse: 7.2629\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2648 - mse: 5.2648 - val_loss: 7.3182 - val_mse: 7.3182\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2393 - mse: 5.2393 - val_loss: 7.3344 - val_mse: 7.3344\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2401 - mse: 5.2401 - val_loss: 7.1435 - val_mse: 7.1435\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2656 - mse: 5.2656 - val_loss: 7.5184 - val_mse: 7.5184\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2587 - mse: 5.2587 - val_loss: 7.4021 - val_mse: 7.4021\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1284 - mse: 5.1284 - val_loss: 7.5323 - val_mse: 7.5323\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1325 - mse: 5.1325 - val_loss: 7.4435 - val_mse: 7.4435\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1493 - mse: 5.1493 - val_loss: 7.4156 - val_mse: 7.4156\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1456 - mse: 5.1456 - val_loss: 7.4604 - val_mse: 7.4604\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1352 - mse: 5.1352 - val_loss: 7.4859 - val_mse: 7.4859\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1161 - mse: 5.1161 - val_loss: 7.3995 - val_mse: 7.3995\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0979 - mse: 5.0979 - val_loss: 7.5734 - val_mse: 7.5734\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1151 - mse: 5.1151 - val_loss: 7.4682 - val_mse: 7.4682\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1726 - mse: 5.1726 - val_loss: 7.4457 - val_mse: 7.4457\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1535 - mse: 5.1535 - val_loss: 7.3898 - val_mse: 7.3898\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0101 - mse: 5.0101 - val_loss: 7.6360 - val_mse: 7.6360\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2980 - mse: 5.2980 - val_loss: 7.5803 - val_mse: 7.5803\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9785 - mse: 4.9785 - val_loss: 7.5349 - val_mse: 7.5349\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0803 - mse: 5.0803 - val_loss: 7.6391 - val_mse: 7.6391\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0486 - mse: 5.0486 - val_loss: 7.5802 - val_mse: 7.5802\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0181 - mse: 5.0181 - val_loss: 7.6617 - val_mse: 7.6617\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2165 - mse: 5.2165 - val_loss: 7.3443 - val_mse: 7.3443\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.9054 - mse: 4.9054 - val_loss: 7.5992 - val_mse: 7.5992\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9447 - mse: 4.9447 - val_loss: 7.6390 - val_mse: 7.6390\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1260 - mse: 5.1260 - val_loss: 7.3340 - val_mse: 7.3340\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9137 - mse: 4.9137 - val_loss: 7.5641 - val_mse: 7.5641\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8840 - mse: 4.8840 - val_loss: 7.4328 - val_mse: 7.4328\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0774 - mse: 5.0774 - val_loss: 7.6314 - val_mse: 7.6314\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9801 - mse: 4.9801 - val_loss: 7.4752 - val_mse: 7.4752\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8802 - mse: 4.8802 - val_loss: 7.5894 - val_mse: 7.5894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b101bbd10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "46f20152-b271-4773-a37d-aba0c46f0045"
      },
      "source": [
        "errors = np.random.randint(5, size = (2, 4))\n",
        "n = len(errors)\n",
        "\n",
        "mse = np.power(errors, 2)/n\n",
        "mse = np.array(mse)\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825af9b9-80c2-4ae6-d712-22b2ad477369"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 22.0643 - mae: 22.0643 - val_loss: 20.5404 - val_mae: 20.5404\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.9907 - mae: 20.9907 - val_loss: 19.2444 - val_mae: 19.2444\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 19.2910 - mae: 19.2910 - val_loss: 17.0491 - val_mae: 17.0491\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.4220 - mae: 16.4220 - val_loss: 13.5261 - val_mae: 13.5261\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.1142 - mae: 12.1142 - val_loss: 8.3248 - val_mae: 8.3248\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5576 - mae: 7.5576 - val_loss: 6.0017 - val_mae: 6.0017\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7403 - mae: 5.7403 - val_loss: 4.8899 - val_mae: 4.8899\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.3540 - mae: 4.3540 - val_loss: 3.9038 - val_mae: 3.9038\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.6122 - mae: 3.6122 - val_loss: 3.5488 - val_mae: 3.5488\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.3688 - mae: 3.3688 - val_loss: 3.3027 - val_mae: 3.3027\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.2343 - mae: 3.2343 - val_loss: 3.2713 - val_mae: 3.2713\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.9776 - mae: 2.9776 - val_loss: 3.0024 - val_mae: 3.0024\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.8468 - mae: 2.8468 - val_loss: 2.9475 - val_mae: 2.9475\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.7179 - mae: 2.7179 - val_loss: 2.7974 - val_mae: 2.7974\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6130 - mae: 2.6130 - val_loss: 2.6683 - val_mae: 2.6683\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.5328 - mae: 2.5328 - val_loss: 2.6040 - val_mae: 2.6040\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4485 - mae: 2.4485 - val_loss: 2.5400 - val_mae: 2.5400\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3853 - mae: 2.3853 - val_loss: 2.4788 - val_mae: 2.4788\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3419 - mae: 2.3419 - val_loss: 2.4923 - val_mae: 2.4923\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3984 - mae: 2.3984 - val_loss: 2.4137 - val_mae: 2.4137\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3491 - mae: 2.3491 - val_loss: 2.3474 - val_mae: 2.3474\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2393 - mae: 2.2393 - val_loss: 2.5127 - val_mae: 2.5127\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1974 - mae: 2.1974 - val_loss: 2.4278 - val_mae: 2.4278\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1621 - mae: 2.1621 - val_loss: 2.4453 - val_mae: 2.4453\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1256 - mae: 2.1256 - val_loss: 2.3410 - val_mae: 2.3410\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1120 - mae: 2.1120 - val_loss: 2.3855 - val_mae: 2.3855\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0996 - mae: 2.0996 - val_loss: 2.3156 - val_mae: 2.3156\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1886 - mae: 2.1886 - val_loss: 2.5139 - val_mae: 2.5139\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1606 - mae: 2.1606 - val_loss: 2.2866 - val_mae: 2.2866\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0961 - mae: 2.0961 - val_loss: 2.2848 - val_mae: 2.2848\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0942 - mae: 2.0942 - val_loss: 2.3467 - val_mae: 2.3467\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0540 - mae: 2.0540 - val_loss: 2.2435 - val_mae: 2.2435\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0090 - mae: 2.0090 - val_loss: 2.3473 - val_mae: 2.3473\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0720 - mae: 2.0720 - val_loss: 2.2926 - val_mae: 2.2926\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0107 - mae: 2.0107 - val_loss: 2.2526 - val_mae: 2.2526\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9445 - mae: 1.9445 - val_loss: 2.2610 - val_mae: 2.2610\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9221 - mae: 1.9221 - val_loss: 2.3066 - val_mae: 2.3066\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9293 - mae: 1.9293 - val_loss: 2.1806 - val_mae: 2.1806\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8897 - mae: 1.8897 - val_loss: 2.2696 - val_mae: 2.2696\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8806 - mae: 1.8806 - val_loss: 2.1458 - val_mae: 2.1458\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8934 - mae: 1.8934 - val_loss: 2.3418 - val_mae: 2.3418\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8756 - mae: 1.8756 - val_loss: 2.1659 - val_mae: 2.1659\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8334 - mae: 1.8334 - val_loss: 2.2457 - val_mae: 2.2457\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8432 - mae: 1.8432 - val_loss: 2.1139 - val_mae: 2.1139\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8451 - mae: 1.8451 - val_loss: 2.2688 - val_mae: 2.2688\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8124 - mae: 1.8124 - val_loss: 2.2059 - val_mae: 2.2059\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7966 - mae: 1.7966 - val_loss: 2.2104 - val_mae: 2.2104\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8016 - mae: 1.8016 - val_loss: 2.1606 - val_mae: 2.1606\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7677 - mae: 1.7677 - val_loss: 2.2537 - val_mae: 2.2537\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7619 - mae: 1.7619 - val_loss: 2.1697 - val_mae: 2.1697\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7582 - mae: 1.7582 - val_loss: 2.2057 - val_mae: 2.2057\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7556 - mae: 1.7556 - val_loss: 2.0197 - val_mae: 2.0197\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8320 - mae: 1.8320 - val_loss: 2.3376 - val_mae: 2.3376\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7793 - mae: 1.7793 - val_loss: 1.9721 - val_mae: 1.9721\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7725 - mae: 1.7725 - val_loss: 2.1593 - val_mae: 2.1593\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7297 - mae: 1.7297 - val_loss: 2.0229 - val_mae: 2.0229\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6858 - mae: 1.6858 - val_loss: 2.1902 - val_mae: 2.1902\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7269 - mae: 1.7269 - val_loss: 2.0987 - val_mae: 2.0987\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6971 - mae: 1.6971 - val_loss: 2.0704 - val_mae: 2.0704\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6470 - mae: 1.6470 - val_loss: 2.0518 - val_mae: 2.0518\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6617 - mae: 1.6617 - val_loss: 2.0889 - val_mae: 2.0889\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6663 - mae: 1.6663 - val_loss: 2.1729 - val_mae: 2.1729\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6461 - mae: 1.6461 - val_loss: 2.0421 - val_mae: 2.0421\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6060 - mae: 1.6060 - val_loss: 1.9340 - val_mae: 1.9340\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6764 - mae: 1.6764 - val_loss: 2.0672 - val_mae: 2.0672\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5960 - mae: 1.5960 - val_loss: 2.1097 - val_mae: 2.1097\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5886 - mae: 1.5886 - val_loss: 2.0034 - val_mae: 2.0034\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5844 - mae: 1.5844 - val_loss: 2.0664 - val_mae: 2.0664\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5799 - mae: 1.5799 - val_loss: 2.0089 - val_mae: 2.0089\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6204 - mae: 1.6204 - val_loss: 2.0676 - val_mae: 2.0676\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5615 - mae: 1.5615 - val_loss: 1.9933 - val_mae: 1.9933\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5567 - mae: 1.5567 - val_loss: 1.9110 - val_mae: 1.9110\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5280 - mae: 1.5280 - val_loss: 2.1002 - val_mae: 2.1002\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5896 - mae: 1.5896 - val_loss: 1.9021 - val_mae: 1.9021\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5727 - mae: 1.5727 - val_loss: 1.9669 - val_mae: 1.9669\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5287 - mae: 1.5287 - val_loss: 2.0550 - val_mae: 2.0550\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5523 - mae: 1.5523 - val_loss: 2.0610 - val_mae: 2.0610\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5685 - mae: 1.5685 - val_loss: 2.0366 - val_mae: 2.0366\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5204 - mae: 1.5204 - val_loss: 1.8937 - val_mae: 1.8937\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4998 - mae: 1.4998 - val_loss: 1.9247 - val_mae: 1.9247\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5436 - mae: 1.5436 - val_loss: 2.0724 - val_mae: 2.0724\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5157 - mae: 1.5157 - val_loss: 1.9106 - val_mae: 1.9106\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5082 - mae: 1.5082 - val_loss: 1.9162 - val_mae: 1.9162\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4782 - mae: 1.4782 - val_loss: 1.9853 - val_mae: 1.9853\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5408 - mae: 1.5408 - val_loss: 1.9942 - val_mae: 1.9942\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5751 - mae: 1.5751 - val_loss: 1.9326 - val_mae: 1.9326\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5316 - mae: 1.5316 - val_loss: 1.9313 - val_mae: 1.9313\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4741 - mae: 1.4741 - val_loss: 1.9500 - val_mae: 1.9500\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4803 - mae: 1.4803 - val_loss: 1.9780 - val_mae: 1.9780\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4977 - mae: 1.4977 - val_loss: 1.8792 - val_mae: 1.8792\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4975 - mae: 1.4975 - val_loss: 2.0071 - val_mae: 2.0071\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4917 - mae: 1.4917 - val_loss: 2.1015 - val_mae: 2.1015\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5770 - mae: 1.5770 - val_loss: 1.8522 - val_mae: 1.8522\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5075 - mae: 1.5075 - val_loss: 1.8683 - val_mae: 1.8683\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4603 - mae: 1.4603 - val_loss: 1.9144 - val_mae: 1.9144\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4371 - mae: 1.4371 - val_loss: 1.9639 - val_mae: 1.9639\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4542 - mae: 1.4542 - val_loss: 1.9008 - val_mae: 1.9008\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4418 - mae: 1.4418 - val_loss: 1.8744 - val_mae: 1.8744\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4222 - mae: 1.4222 - val_loss: 1.9987 - val_mae: 1.9987\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4232 - mae: 1.4232 - val_loss: 1.9215 - val_mae: 1.9215\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4159 - mae: 1.4159 - val_loss: 1.9844 - val_mae: 1.9844\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4334 - mae: 1.4334 - val_loss: 1.8674 - val_mae: 1.8674\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4181 - mae: 1.4181 - val_loss: 1.8651 - val_mae: 1.8651\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3948 - mae: 1.3948 - val_loss: 1.8941 - val_mae: 1.8941\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3948 - mae: 1.3948 - val_loss: 1.8200 - val_mae: 1.8200\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4645 - mae: 1.4645 - val_loss: 1.8467 - val_mae: 1.8467\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4340 - mae: 1.4340 - val_loss: 1.8426 - val_mae: 1.8426\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3914 - mae: 1.3914 - val_loss: 1.8926 - val_mae: 1.8926\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3745 - mae: 1.3745 - val_loss: 1.8122 - val_mae: 1.8122\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3905 - mae: 1.3905 - val_loss: 1.8477 - val_mae: 1.8477\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4934 - mae: 1.4934 - val_loss: 1.7521 - val_mae: 1.7521\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3841 - mae: 1.3841 - val_loss: 1.8609 - val_mae: 1.8609\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3696 - mae: 1.3696 - val_loss: 1.8237 - val_mae: 1.8237\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4105 - mae: 1.4105 - val_loss: 1.8771 - val_mae: 1.8771\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3993 - mae: 1.3993 - val_loss: 1.9302 - val_mae: 1.9302\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4306 - mae: 1.4306 - val_loss: 1.8380 - val_mae: 1.8380\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3638 - mae: 1.3638 - val_loss: 1.8006 - val_mae: 1.8006\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3786 - mae: 1.3786 - val_loss: 1.8073 - val_mae: 1.8073\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3564 - mae: 1.3564 - val_loss: 1.8502 - val_mae: 1.8502\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4029 - mae: 1.4029 - val_loss: 1.7754 - val_mae: 1.7754\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5035 - mae: 1.5035 - val_loss: 2.0628 - val_mae: 2.0628\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4584 - mae: 1.4584 - val_loss: 1.8349 - val_mae: 1.8349\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4698 - mae: 1.4698 - val_loss: 1.8125 - val_mae: 1.8125\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3712 - mae: 1.3712 - val_loss: 1.7706 - val_mae: 1.7706\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3695 - mae: 1.3695 - val_loss: 1.8184 - val_mae: 1.8184\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3188 - mae: 1.3188 - val_loss: 1.9057 - val_mae: 1.9057\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3553 - mae: 1.3553 - val_loss: 1.9177 - val_mae: 1.9177\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3573 - mae: 1.3573 - val_loss: 1.9166 - val_mae: 1.9166\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3319 - mae: 1.3319 - val_loss: 1.8182 - val_mae: 1.8182\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2890 - mae: 1.2890 - val_loss: 1.9225 - val_mae: 1.9225\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5398 - mae: 1.5398 - val_loss: 1.8298 - val_mae: 1.8298\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4396 - mae: 1.4396 - val_loss: 1.8749 - val_mae: 1.8749\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3159 - mae: 1.3159 - val_loss: 1.8960 - val_mae: 1.8960\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3058 - mae: 1.3058 - val_loss: 1.7622 - val_mae: 1.7622\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3009 - mae: 1.3009 - val_loss: 1.9828 - val_mae: 1.9828\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3909 - mae: 1.3909 - val_loss: 1.8088 - val_mae: 1.8088\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2808 - mae: 1.2808 - val_loss: 1.7868 - val_mae: 1.7868\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2557 - mae: 1.2557 - val_loss: 1.8136 - val_mae: 1.8136\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2772 - mae: 1.2772 - val_loss: 1.7785 - val_mae: 1.7785\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2623 - mae: 1.2623 - val_loss: 1.8015 - val_mae: 1.8015\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3035 - mae: 1.3035 - val_loss: 1.9414 - val_mae: 1.9414\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3379 - mae: 1.3379 - val_loss: 1.8399 - val_mae: 1.8399\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3277 - mae: 1.3277 - val_loss: 1.8090 - val_mae: 1.8090\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3110 - mae: 1.3110 - val_loss: 1.7671 - val_mae: 1.7671\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3686 - mae: 1.3686 - val_loss: 1.9196 - val_mae: 1.9196\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2784 - mae: 1.2784 - val_loss: 1.8485 - val_mae: 1.8485\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2572 - mae: 1.2572 - val_loss: 1.7766 - val_mae: 1.7766\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2931 - mae: 1.2931 - val_loss: 1.7831 - val_mae: 1.7831\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2858 - mae: 1.2858 - val_loss: 1.7889 - val_mae: 1.7889\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2783 - mae: 1.2783 - val_loss: 1.8711 - val_mae: 1.8711\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2531 - mae: 1.2531 - val_loss: 1.8394 - val_mae: 1.8394\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2300 - mae: 1.2300 - val_loss: 1.7681 - val_mae: 1.7681\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3430 - mae: 1.3430 - val_loss: 1.8783 - val_mae: 1.8783\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2275 - mae: 1.2275 - val_loss: 1.8886 - val_mae: 1.8886\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2293 - mae: 1.2293 - val_loss: 1.7978 - val_mae: 1.7978\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2263 - mae: 1.2263 - val_loss: 1.8321 - val_mae: 1.8321\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2140 - mae: 1.2140 - val_loss: 1.8360 - val_mae: 1.8360\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2212 - mae: 1.2212 - val_loss: 1.8204 - val_mae: 1.8204\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2574 - mae: 1.2574 - val_loss: 1.7717 - val_mae: 1.7717\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2521 - mae: 1.2521 - val_loss: 1.8153 - val_mae: 1.8153\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2303 - mae: 1.2303 - val_loss: 1.8037 - val_mae: 1.8037\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2517 - mae: 1.2517 - val_loss: 1.7666 - val_mae: 1.7666\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2936 - mae: 1.2936 - val_loss: 1.8671 - val_mae: 1.8671\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2150 - mae: 1.2150 - val_loss: 1.8135 - val_mae: 1.8135\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2055 - mae: 1.2055 - val_loss: 1.7895 - val_mae: 1.7895\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2036 - mae: 1.2036 - val_loss: 1.7945 - val_mae: 1.7945\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2624 - mae: 1.2624 - val_loss: 1.8045 - val_mae: 1.8045\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2027 - mae: 1.2027 - val_loss: 1.7961 - val_mae: 1.7961\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2131 - mae: 1.2131 - val_loss: 1.8284 - val_mae: 1.8284\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2958 - mae: 1.2958 - val_loss: 1.7917 - val_mae: 1.7917\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2484 - mae: 1.2484 - val_loss: 1.7549 - val_mae: 1.7549\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2802 - mae: 1.2802 - val_loss: 1.8085 - val_mae: 1.8085\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2558 - mae: 1.2558 - val_loss: 1.9478 - val_mae: 1.9478\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2396 - mae: 1.2396 - val_loss: 1.7288 - val_mae: 1.7288\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2124 - mae: 1.2124 - val_loss: 1.7840 - val_mae: 1.7840\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1925 - mae: 1.1925 - val_loss: 1.7345 - val_mae: 1.7345\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2294 - mae: 1.2294 - val_loss: 1.7457 - val_mae: 1.7457\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1897 - mae: 1.1897 - val_loss: 1.8174 - val_mae: 1.8174\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2045 - mae: 1.2045 - val_loss: 1.8593 - val_mae: 1.8593\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2384 - mae: 1.2384 - val_loss: 1.7820 - val_mae: 1.7820\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2103 - mae: 1.2103 - val_loss: 1.7145 - val_mae: 1.7145\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1752 - mae: 1.1752 - val_loss: 1.7015 - val_mae: 1.7015\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1638 - mae: 1.1638 - val_loss: 1.7543 - val_mae: 1.7543\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1542 - mae: 1.1542 - val_loss: 1.7934 - val_mae: 1.7934\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.1714 - val_loss: 1.7754 - val_mae: 1.7754\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.1714 - val_loss: 1.7407 - val_mae: 1.7407\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1547 - mae: 1.1547 - val_loss: 1.7642 - val_mae: 1.7642\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1277 - mae: 1.1277 - val_loss: 1.7725 - val_mae: 1.7725\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1275 - mae: 1.1275 - val_loss: 1.7422 - val_mae: 1.7422\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1317 - mae: 1.1317 - val_loss: 1.7943 - val_mae: 1.7943\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2173 - mae: 1.2173 - val_loss: 1.7676 - val_mae: 1.7676\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1748 - mae: 1.1748 - val_loss: 1.7983 - val_mae: 1.7983\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1661 - mae: 1.1661 - val_loss: 1.7196 - val_mae: 1.7196\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1741 - mae: 1.1741 - val_loss: 1.7702 - val_mae: 1.7702\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1132 - mae: 1.1132 - val_loss: 1.7666 - val_mae: 1.7666\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1256 - mae: 1.1256 - val_loss: 1.7878 - val_mae: 1.7878\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1268 - mae: 1.1268 - val_loss: 1.7268 - val_mae: 1.7268\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1442 - mae: 1.1442 - val_loss: 1.7587 - val_mae: 1.7587\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1164 - mae: 1.1164 - val_loss: 1.8000 - val_mae: 1.8000\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1465 - mae: 1.1465 - val_loss: 1.7433 - val_mae: 1.7433\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1082 - mae: 1.1082 - val_loss: 1.7478 - val_mae: 1.7478\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1095 - mae: 1.1095 - val_loss: 1.8166 - val_mae: 1.8166\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1143 - mae: 1.1143 - val_loss: 1.7282 - val_mae: 1.7282\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1091 - mae: 1.1091 - val_loss: 1.7232 - val_mae: 1.7232\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0835 - mae: 1.0835 - val_loss: 1.7333 - val_mae: 1.7333\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1354 - mae: 1.1354 - val_loss: 1.7330 - val_mae: 1.7330\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1625 - mae: 1.1625 - val_loss: 1.8246 - val_mae: 1.8246\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1573 - mae: 1.1573 - val_loss: 1.7362 - val_mae: 1.7362\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1350 - mae: 1.1350 - val_loss: 1.6828 - val_mae: 1.6828\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1183 - mae: 1.1183 - val_loss: 1.7214 - val_mae: 1.7214\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1253 - mae: 1.1253 - val_loss: 1.7119 - val_mae: 1.7119\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2101 - mae: 1.2101 - val_loss: 1.8696 - val_mae: 1.8696\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1628 - mae: 1.1628 - val_loss: 1.7588 - val_mae: 1.7588\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1985 - mae: 1.1985 - val_loss: 1.7232 - val_mae: 1.7232\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0899 - mae: 1.0899 - val_loss: 1.7893 - val_mae: 1.7893\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1082 - mae: 1.1082 - val_loss: 1.7662 - val_mae: 1.7662\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1105 - mae: 1.1105 - val_loss: 1.7061 - val_mae: 1.7061\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1142 - mae: 1.1142 - val_loss: 1.6790 - val_mae: 1.6790\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1498 - mae: 1.1498 - val_loss: 1.6816 - val_mae: 1.6816\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1721 - mae: 1.1721 - val_loss: 1.7320 - val_mae: 1.7320\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1536 - mae: 1.1536 - val_loss: 1.8030 - val_mae: 1.8030\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1087 - mae: 1.1087 - val_loss: 1.7144 - val_mae: 1.7144\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0745 - mae: 1.0745 - val_loss: 1.6979 - val_mae: 1.6979\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0940 - mae: 1.0940 - val_loss: 1.6908 - val_mae: 1.6908\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0879 - mae: 1.0879 - val_loss: 1.6646 - val_mae: 1.6646\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0626 - mae: 1.0626 - val_loss: 1.7174 - val_mae: 1.7174\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0811 - mae: 1.0811 - val_loss: 1.6654 - val_mae: 1.6654\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0866 - mae: 1.0866 - val_loss: 1.6718 - val_mae: 1.6718\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0814 - mae: 1.0814 - val_loss: 1.6741 - val_mae: 1.6741\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0487 - mae: 1.0487 - val_loss: 1.6950 - val_mae: 1.6950\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0636 - mae: 1.0636 - val_loss: 1.6564 - val_mae: 1.6564\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0951 - mae: 1.0951 - val_loss: 1.7192 - val_mae: 1.7192\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1567 - mae: 1.1567 - val_loss: 1.6414 - val_mae: 1.6414\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1032 - mae: 1.1032 - val_loss: 1.7026 - val_mae: 1.7026\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0994 - mae: 1.0994 - val_loss: 1.6583 - val_mae: 1.6583\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1444 - mae: 1.1444 - val_loss: 1.7397 - val_mae: 1.7397\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1404 - mae: 1.1404 - val_loss: 1.7503 - val_mae: 1.7503\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0823 - mae: 1.0823 - val_loss: 1.6899 - val_mae: 1.6899\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0919 - mae: 1.0919 - val_loss: 1.6385 - val_mae: 1.6385\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0554 - mae: 1.0554 - val_loss: 1.6821 - val_mae: 1.6821\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0483 - mae: 1.0483 - val_loss: 1.6477 - val_mae: 1.6477\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0471 - mae: 1.0471 - val_loss: 1.6683 - val_mae: 1.6683\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0958 - mae: 1.0958 - val_loss: 1.7186 - val_mae: 1.7186\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1386 - mae: 1.1386 - val_loss: 1.7917 - val_mae: 1.7917\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0577 - mae: 1.0577 - val_loss: 1.6438 - val_mae: 1.6438\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0764 - mae: 1.0764 - val_loss: 1.6538 - val_mae: 1.6538\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0560 - mae: 1.0560 - val_loss: 1.7119 - val_mae: 1.7119\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1083 - mae: 1.1083 - val_loss: 1.6248 - val_mae: 1.6248\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0830 - mae: 1.0830 - val_loss: 1.6391 - val_mae: 1.6391\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1329 - mae: 1.1329 - val_loss: 1.6744 - val_mae: 1.6744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83b00875d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a4ed8f05-4722-4098-ad46-6b77723aeace"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "mae = np.absolute(errors)/n\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9210ae2d-efed-417f-cdc3-644edf230e3d"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 7.8223 - mean_squared_logarithmic_error: 7.8223 - val_loss: 5.9455 - val_mean_squared_logarithmic_error: 5.9455\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0239 - mean_squared_logarithmic_error: 5.0239 - val_loss: 3.6531 - val_mean_squared_logarithmic_error: 3.6531\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.0678 - mean_squared_logarithmic_error: 3.0678 - val_loss: 2.1314 - val_mean_squared_logarithmic_error: 2.1314\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7921 - mean_squared_logarithmic_error: 1.7921 - val_loss: 1.2186 - val_mean_squared_logarithmic_error: 1.2186\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0475 - mean_squared_logarithmic_error: 1.0475 - val_loss: 0.6925 - val_mean_squared_logarithmic_error: 0.6925\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6128 - mean_squared_logarithmic_error: 0.6128 - val_loss: 0.3870 - val_mean_squared_logarithmic_error: 0.3870\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3556 - mean_squared_logarithmic_error: 0.3556 - val_loss: 0.2196 - val_mean_squared_logarithmic_error: 0.2196\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2196 - mean_squared_logarithmic_error: 0.2196 - val_loss: 0.1408 - val_mean_squared_logarithmic_error: 0.1408\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1544 - mean_squared_logarithmic_error: 0.1544 - val_loss: 0.1064 - val_mean_squared_logarithmic_error: 0.1064\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1234 - mean_squared_logarithmic_error: 0.1234 - val_loss: 0.0889 - val_mean_squared_logarithmic_error: 0.0889\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1065 - mean_squared_logarithmic_error: 0.1065 - val_loss: 0.0788 - val_mean_squared_logarithmic_error: 0.0788\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0957 - mean_squared_logarithmic_error: 0.0957 - val_loss: 0.0719 - val_mean_squared_logarithmic_error: 0.0719\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0875 - mean_squared_logarithmic_error: 0.0875 - val_loss: 0.0667 - val_mean_squared_logarithmic_error: 0.0667\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0813 - mean_squared_logarithmic_error: 0.0813 - val_loss: 0.0617 - val_mean_squared_logarithmic_error: 0.0617\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0761 - mean_squared_logarithmic_error: 0.0761 - val_loss: 0.0567 - val_mean_squared_logarithmic_error: 0.0567\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0711 - mean_squared_logarithmic_error: 0.0711 - val_loss: 0.0530 - val_mean_squared_logarithmic_error: 0.0530\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_squared_logarithmic_error: 0.0669 - val_loss: 0.0497 - val_mean_squared_logarithmic_error: 0.0497\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0629 - mean_squared_logarithmic_error: 0.0629 - val_loss: 0.0470 - val_mean_squared_logarithmic_error: 0.0470\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0594 - mean_squared_logarithmic_error: 0.0594 - val_loss: 0.0445 - val_mean_squared_logarithmic_error: 0.0445\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0560 - mean_squared_logarithmic_error: 0.0560 - val_loss: 0.0418 - val_mean_squared_logarithmic_error: 0.0418\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0532 - mean_squared_logarithmic_error: 0.0532 - val_loss: 0.0396 - val_mean_squared_logarithmic_error: 0.0396\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0505 - mean_squared_logarithmic_error: 0.0505 - val_loss: 0.0380 - val_mean_squared_logarithmic_error: 0.0380\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0482 - mean_squared_logarithmic_error: 0.0482 - val_loss: 0.0367 - val_mean_squared_logarithmic_error: 0.0367\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0463 - mean_squared_logarithmic_error: 0.0463 - val_loss: 0.0357 - val_mean_squared_logarithmic_error: 0.0357\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0444 - mean_squared_logarithmic_error: 0.0444 - val_loss: 0.0350 - val_mean_squared_logarithmic_error: 0.0350\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0429 - mean_squared_logarithmic_error: 0.0429 - val_loss: 0.0341 - val_mean_squared_logarithmic_error: 0.0341\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0414 - mean_squared_logarithmic_error: 0.0414 - val_loss: 0.0338 - val_mean_squared_logarithmic_error: 0.0338\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0400 - mean_squared_logarithmic_error: 0.0400 - val_loss: 0.0336 - val_mean_squared_logarithmic_error: 0.0336\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0388 - mean_squared_logarithmic_error: 0.0388 - val_loss: 0.0330 - val_mean_squared_logarithmic_error: 0.0330\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0379 - mean_squared_logarithmic_error: 0.0379 - val_loss: 0.0331 - val_mean_squared_logarithmic_error: 0.0331\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0367 - mean_squared_logarithmic_error: 0.0367 - val_loss: 0.0325 - val_mean_squared_logarithmic_error: 0.0325\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0358 - mean_squared_logarithmic_error: 0.0358 - val_loss: 0.0317 - val_mean_squared_logarithmic_error: 0.0317\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0351 - mean_squared_logarithmic_error: 0.0351 - val_loss: 0.0313 - val_mean_squared_logarithmic_error: 0.0313\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0342 - mean_squared_logarithmic_error: 0.0342 - val_loss: 0.0311 - val_mean_squared_logarithmic_error: 0.0311\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0335 - mean_squared_logarithmic_error: 0.0335 - val_loss: 0.0306 - val_mean_squared_logarithmic_error: 0.0306\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0328 - mean_squared_logarithmic_error: 0.0328 - val_loss: 0.0303 - val_mean_squared_logarithmic_error: 0.0303\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0322 - mean_squared_logarithmic_error: 0.0322 - val_loss: 0.0296 - val_mean_squared_logarithmic_error: 0.0296\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0317 - mean_squared_logarithmic_error: 0.0317 - val_loss: 0.0293 - val_mean_squared_logarithmic_error: 0.0293\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0311 - mean_squared_logarithmic_error: 0.0311 - val_loss: 0.0295 - val_mean_squared_logarithmic_error: 0.0295\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0303 - mean_squared_logarithmic_error: 0.0303 - val_loss: 0.0285 - val_mean_squared_logarithmic_error: 0.0285\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0300 - mean_squared_logarithmic_error: 0.0300 - val_loss: 0.0281 - val_mean_squared_logarithmic_error: 0.0281\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0296 - mean_squared_logarithmic_error: 0.0296 - val_loss: 0.0275 - val_mean_squared_logarithmic_error: 0.0275\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0289 - mean_squared_logarithmic_error: 0.0289 - val_loss: 0.0278 - val_mean_squared_logarithmic_error: 0.0278\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - mean_squared_logarithmic_error: 0.0286 - val_loss: 0.0272 - val_mean_squared_logarithmic_error: 0.0272\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0281 - mean_squared_logarithmic_error: 0.0281 - val_loss: 0.0273 - val_mean_squared_logarithmic_error: 0.0273\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - mean_squared_logarithmic_error: 0.0276 - val_loss: 0.0270 - val_mean_squared_logarithmic_error: 0.0270\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0266 - val_mean_squared_logarithmic_error: 0.0266\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0269 - mean_squared_logarithmic_error: 0.0269 - val_loss: 0.0258 - val_mean_squared_logarithmic_error: 0.0258\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0264 - mean_squared_logarithmic_error: 0.0264 - val_loss: 0.0258 - val_mean_squared_logarithmic_error: 0.0258\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - mean_squared_logarithmic_error: 0.0260 - val_loss: 0.0260 - val_mean_squared_logarithmic_error: 0.0260\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0257 - val_mean_squared_logarithmic_error: 0.0257\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - mean_squared_logarithmic_error: 0.0252 - val_loss: 0.0247 - val_mean_squared_logarithmic_error: 0.0247\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - mean_squared_logarithmic_error: 0.0249 - val_loss: 0.0243 - val_mean_squared_logarithmic_error: 0.0243\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - mean_squared_logarithmic_error: 0.0245 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0243 - mean_squared_logarithmic_error: 0.0243 - val_loss: 0.0243 - val_mean_squared_logarithmic_error: 0.0243\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - mean_squared_logarithmic_error: 0.0240 - val_loss: 0.0240 - val_mean_squared_logarithmic_error: 0.0240\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - mean_squared_logarithmic_error: 0.0239 - val_loss: 0.0234 - val_mean_squared_logarithmic_error: 0.0234\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - mean_squared_logarithmic_error: 0.0233 - val_loss: 0.0238 - val_mean_squared_logarithmic_error: 0.0238\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - mean_squared_logarithmic_error: 0.0231 - val_loss: 0.0236 - val_mean_squared_logarithmic_error: 0.0236\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - mean_squared_logarithmic_error: 0.0228 - val_loss: 0.0232 - val_mean_squared_logarithmic_error: 0.0232\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0230 - val_mean_squared_logarithmic_error: 0.0230\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - mean_squared_logarithmic_error: 0.0222 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - mean_squared_logarithmic_error: 0.0217 - val_loss: 0.0227 - val_mean_squared_logarithmic_error: 0.0227\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0219 - val_mean_squared_logarithmic_error: 0.0219\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0208 - mean_squared_logarithmic_error: 0.0208 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - mean_squared_logarithmic_error: 0.0200 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - mean_squared_logarithmic_error: 0.0189 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - mean_squared_logarithmic_error: 0.0189 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0180 - val_mean_squared_logarithmic_error: 0.0180\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0150 - mean_squared_logarithmic_error: 0.0150 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0148 - mean_squared_logarithmic_error: 0.0148 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0150 - mean_squared_logarithmic_error: 0.0150 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0143 - mean_squared_logarithmic_error: 0.0143 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - mean_squared_logarithmic_error: 0.0143 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0142 - mean_squared_logarithmic_error: 0.0142 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8346234510>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b6da8dac-fe6e-4228-91b3-6fbc2c2fcaed"
      },
      "source": [
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "msle = np.power(np.log(actual_outputs+1) - np.log(estimated_outputs+1),2)/n\n",
        "\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n",
        "## Answer 4\n",
        "We don't want to take the $\\log(0)$ because the result is undefined, so we add $1$ to make sure all the values are defined ($\\log(1)$ is 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n",
        "\n",
        "## Answer 5\n",
        "MSE is quadratic and forms a parabolic curve, and thus, punishes larger errors/deviations significantly more harshly, at an increasing rate.\n",
        "\n",
        "MAE forms 2 straight lines, so deviations (larger and smaller) are punished on a linear scale. Outliers are still punished more than closer values, but much less harshly.\n",
        "\n",
        "MSLE forms a logarithmic curve, so larger deviations result in a larger loss value, but less so than in MSE or MAE, and small differences are punished very lightly (because of the $+1$). Furthermore, the larger the deviation, the less the increase in loss. AS a result, larger loss values tend to have a similar penalty to smaller ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n",
        "\n",
        "## Answer 6\n",
        "The model using MAPE performs significantly worse in terms of loss when compared to the other loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b34ca2-3d4f-4aa3-e75b-005803e9974a"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= \"MAPE\",\n",
        "              # metrics=[\"MAPE\", \"mse\", \"mae\", \"msle\"])\n",
        "              metrics=[\"MAPE\"])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 23ms/step - loss: 98.3630 - MAPE: 98.3630 - val_loss: 95.4736 - val_MAPE: 95.4736\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 93.4244 - MAPE: 93.4244 - val_loss: 88.5860 - val_MAPE: 88.5860\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 84.2524 - MAPE: 84.2524 - val_loss: 75.1924 - val_MAPE: 75.1924\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 67.4964 - MAPE: 67.4964 - val_loss: 56.8033 - val_MAPE: 56.8033\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 48.1859 - MAPE: 48.1859 - val_loss: 36.8880 - val_MAPE: 36.8880\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 32.4676 - MAPE: 32.4676 - val_loss: 28.6129 - val_MAPE: 28.6129\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 27.0349 - MAPE: 27.0349 - val_loss: 23.1867 - val_MAPE: 23.1867\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.0443 - MAPE: 22.0443 - val_loss: 19.6820 - val_MAPE: 19.6820\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.3318 - MAPE: 19.3318 - val_loss: 17.9720 - val_MAPE: 17.9720\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.0595 - MAPE: 17.0595 - val_loss: 17.5211 - val_MAPE: 17.5211\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.8450 - MAPE: 15.8450 - val_loss: 16.4390 - val_MAPE: 16.4390\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.9577 - MAPE: 14.9577 - val_loss: 15.6967 - val_MAPE: 15.6967\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.4351 - MAPE: 14.4351 - val_loss: 15.6777 - val_MAPE: 15.6777\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7533 - MAPE: 13.7533 - val_loss: 14.1579 - val_MAPE: 14.1579\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.3434 - MAPE: 13.3434 - val_loss: 14.8125 - val_MAPE: 14.8125\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.3078 - MAPE: 13.3078 - val_loss: 13.6533 - val_MAPE: 13.6533\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6907 - MAPE: 12.6907 - val_loss: 14.4995 - val_MAPE: 14.4995\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.4346 - MAPE: 12.4346 - val_loss: 13.8945 - val_MAPE: 13.8945\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7116 - MAPE: 11.7116 - val_loss: 12.9116 - val_MAPE: 12.9116\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.3928 - MAPE: 11.3928 - val_loss: 13.5115 - val_MAPE: 13.5115\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.0323 - MAPE: 11.0323 - val_loss: 12.9077 - val_MAPE: 12.9077\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7911 - MAPE: 10.7911 - val_loss: 13.2497 - val_MAPE: 13.2497\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.6955 - MAPE: 10.6955 - val_loss: 13.0312 - val_MAPE: 13.0312\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6346 - MAPE: 10.6346 - val_loss: 12.2911 - val_MAPE: 12.2911\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.5098 - MAPE: 10.5098 - val_loss: 12.7444 - val_MAPE: 12.7444\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1864 - MAPE: 10.1864 - val_loss: 12.3168 - val_MAPE: 12.3168\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0913 - MAPE: 10.0913 - val_loss: 13.1276 - val_MAPE: 13.1276\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0991 - MAPE: 10.0991 - val_loss: 11.9130 - val_MAPE: 11.9130\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9700 - MAPE: 9.9700 - val_loss: 12.6687 - val_MAPE: 12.6687\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9657 - MAPE: 9.9657 - val_loss: 12.0478 - val_MAPE: 12.0478\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7468 - MAPE: 9.7468 - val_loss: 12.7449 - val_MAPE: 12.7449\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7657 - MAPE: 9.7657 - val_loss: 11.6479 - val_MAPE: 11.6479\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8944 - MAPE: 9.8944 - val_loss: 12.9187 - val_MAPE: 12.9187\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7833 - MAPE: 9.7833 - val_loss: 12.0085 - val_MAPE: 12.0085\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6873 - MAPE: 9.6873 - val_loss: 13.2664 - val_MAPE: 13.2664\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.5833 - MAPE: 9.5833 - val_loss: 12.1343 - val_MAPE: 12.1343\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5846 - MAPE: 9.5846 - val_loss: 12.3092 - val_MAPE: 12.3092\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2183 - MAPE: 9.2183 - val_loss: 11.7785 - val_MAPE: 11.7785\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3435 - MAPE: 9.3435 - val_loss: 12.2030 - val_MAPE: 12.2030\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1268 - MAPE: 9.1268 - val_loss: 12.1696 - val_MAPE: 12.1696\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2862 - MAPE: 9.2862 - val_loss: 11.9113 - val_MAPE: 11.9113\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0819 - MAPE: 9.0819 - val_loss: 12.1388 - val_MAPE: 12.1388\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0696 - MAPE: 9.0696 - val_loss: 11.6795 - val_MAPE: 11.6795\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8935 - MAPE: 8.8935 - val_loss: 12.0411 - val_MAPE: 12.0411\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8304 - MAPE: 8.8304 - val_loss: 11.7925 - val_MAPE: 11.7925\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0100 - MAPE: 9.0100 - val_loss: 12.4983 - val_MAPE: 12.4983\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8240 - MAPE: 8.8240 - val_loss: 12.0810 - val_MAPE: 12.0810\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9976 - MAPE: 8.9976 - val_loss: 11.0223 - val_MAPE: 11.0223\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2987 - MAPE: 9.2987 - val_loss: 12.7871 - val_MAPE: 12.7871\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9608 - MAPE: 8.9608 - val_loss: 11.1815 - val_MAPE: 11.1815\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2419 - MAPE: 9.2419 - val_loss: 11.7911 - val_MAPE: 11.7911\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6249 - MAPE: 8.6249 - val_loss: 11.8442 - val_MAPE: 11.8442\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4632 - MAPE: 8.4632 - val_loss: 11.6381 - val_MAPE: 11.6381\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6149 - MAPE: 8.6149 - val_loss: 12.3959 - val_MAPE: 12.3959\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5238 - MAPE: 8.5238 - val_loss: 11.2288 - val_MAPE: 11.2288\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3769 - MAPE: 8.3769 - val_loss: 12.6279 - val_MAPE: 12.6279\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4658 - MAPE: 8.4658 - val_loss: 11.5757 - val_MAPE: 11.5757\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9436 - MAPE: 8.9436 - val_loss: 12.6113 - val_MAPE: 12.6113\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9403 - MAPE: 8.9403 - val_loss: 11.5898 - val_MAPE: 11.5898\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3845 - MAPE: 8.3845 - val_loss: 11.5802 - val_MAPE: 11.5802\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1633 - MAPE: 8.1633 - val_loss: 11.8170 - val_MAPE: 11.8170\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0893 - MAPE: 8.0893 - val_loss: 11.7901 - val_MAPE: 11.7901\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0754 - MAPE: 8.0754 - val_loss: 11.6049 - val_MAPE: 11.6049\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0603 - MAPE: 8.0603 - val_loss: 11.4977 - val_MAPE: 11.4977\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2941 - MAPE: 8.2941 - val_loss: 12.1954 - val_MAPE: 12.1954\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8778 - MAPE: 7.8778 - val_loss: 11.8695 - val_MAPE: 11.8695\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9567 - MAPE: 7.9567 - val_loss: 11.6476 - val_MAPE: 11.6476\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0184 - MAPE: 8.0184 - val_loss: 11.6167 - val_MAPE: 11.6167\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7538 - MAPE: 7.7538 - val_loss: 11.9271 - val_MAPE: 11.9271\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7704 - MAPE: 7.7704 - val_loss: 12.3067 - val_MAPE: 12.3067\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8208 - MAPE: 7.8208 - val_loss: 11.5746 - val_MAPE: 11.5746\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6992 - MAPE: 7.6992 - val_loss: 11.5092 - val_MAPE: 11.5092\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5487 - MAPE: 7.5487 - val_loss: 11.9519 - val_MAPE: 11.9519\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7553 - MAPE: 7.7553 - val_loss: 11.4771 - val_MAPE: 11.4771\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8088 - MAPE: 7.8088 - val_loss: 11.6751 - val_MAPE: 11.6751\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8980 - MAPE: 7.8980 - val_loss: 11.9554 - val_MAPE: 11.9554\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6416 - MAPE: 7.6416 - val_loss: 10.8480 - val_MAPE: 10.8480\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6358 - MAPE: 7.6358 - val_loss: 11.6627 - val_MAPE: 11.6627\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5490 - MAPE: 7.5490 - val_loss: 11.7523 - val_MAPE: 11.7523\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4332 - MAPE: 7.4332 - val_loss: 11.0744 - val_MAPE: 11.0744\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7374 - MAPE: 7.7374 - val_loss: 12.1297 - val_MAPE: 12.1297\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6207 - MAPE: 7.6207 - val_loss: 11.1463 - val_MAPE: 11.1463\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6135 - MAPE: 7.6135 - val_loss: 11.7509 - val_MAPE: 11.7509\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5134 - MAPE: 7.5134 - val_loss: 11.4322 - val_MAPE: 11.4322\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3350 - MAPE: 7.3350 - val_loss: 11.4169 - val_MAPE: 11.4169\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2279 - MAPE: 7.2279 - val_loss: 11.3187 - val_MAPE: 11.3187\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3217 - MAPE: 7.3217 - val_loss: 11.6443 - val_MAPE: 11.6443\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2788 - MAPE: 7.2788 - val_loss: 11.6424 - val_MAPE: 11.6424\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1507 - MAPE: 7.1507 - val_loss: 11.7399 - val_MAPE: 11.7399\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4119 - MAPE: 7.4119 - val_loss: 11.3201 - val_MAPE: 11.3201\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2047 - MAPE: 7.2047 - val_loss: 11.1916 - val_MAPE: 11.1916\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0017 - MAPE: 7.0017 - val_loss: 11.5188 - val_MAPE: 11.5188\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0196 - MAPE: 7.0196 - val_loss: 11.2946 - val_MAPE: 11.2946\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0363 - MAPE: 7.0363 - val_loss: 11.6525 - val_MAPE: 11.6525\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0533 - MAPE: 7.0533 - val_loss: 11.6265 - val_MAPE: 11.6265\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1218 - MAPE: 7.1218 - val_loss: 10.9918 - val_MAPE: 10.9918\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2464 - MAPE: 7.2464 - val_loss: 10.8040 - val_MAPE: 10.8040\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0268 - MAPE: 7.0268 - val_loss: 11.2308 - val_MAPE: 11.2308\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9782 - MAPE: 6.9782 - val_loss: 11.4203 - val_MAPE: 11.4203\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8317 - MAPE: 6.8317 - val_loss: 11.0621 - val_MAPE: 11.0621\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8614 - MAPE: 6.8614 - val_loss: 10.6621 - val_MAPE: 10.6621\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1959 - MAPE: 7.1959 - val_loss: 11.1889 - val_MAPE: 11.1889\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1197 - MAPE: 7.1197 - val_loss: 10.9543 - val_MAPE: 10.9543\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3910 - MAPE: 7.3910 - val_loss: 11.5277 - val_MAPE: 11.5277\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9253 - MAPE: 6.9253 - val_loss: 11.0616 - val_MAPE: 11.0616\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9062 - MAPE: 6.9062 - val_loss: 11.3332 - val_MAPE: 11.3332\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7786 - MAPE: 6.7786 - val_loss: 11.1148 - val_MAPE: 11.1148\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8373 - MAPE: 6.8373 - val_loss: 10.7872 - val_MAPE: 10.7872\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0142 - MAPE: 7.0142 - val_loss: 10.8210 - val_MAPE: 10.8210\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6937 - MAPE: 6.6937 - val_loss: 11.3226 - val_MAPE: 11.3226\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9167 - MAPE: 6.9167 - val_loss: 11.2425 - val_MAPE: 11.2425\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6529 - MAPE: 6.6529 - val_loss: 10.7630 - val_MAPE: 10.7630\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7558 - MAPE: 6.7558 - val_loss: 11.0668 - val_MAPE: 11.0668\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7148 - MAPE: 6.7148 - val_loss: 10.8067 - val_MAPE: 10.8067\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6033 - MAPE: 6.6033 - val_loss: 11.0968 - val_MAPE: 11.0968\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7993 - MAPE: 6.7993 - val_loss: 10.8028 - val_MAPE: 10.8028\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2181 - MAPE: 7.2181 - val_loss: 10.5719 - val_MAPE: 10.5719\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9787 - MAPE: 6.9787 - val_loss: 10.9487 - val_MAPE: 10.9487\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7103 - MAPE: 6.7103 - val_loss: 10.9923 - val_MAPE: 10.9923\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7264 - MAPE: 6.7264 - val_loss: 11.2828 - val_MAPE: 11.2828\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7046 - MAPE: 6.7046 - val_loss: 11.3087 - val_MAPE: 11.3087\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9530 - MAPE: 6.9530 - val_loss: 10.4514 - val_MAPE: 10.4514\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6875 - MAPE: 6.6875 - val_loss: 10.7600 - val_MAPE: 10.7600\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7165 - MAPE: 6.7165 - val_loss: 10.9380 - val_MAPE: 10.9380\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8111 - MAPE: 6.8111 - val_loss: 10.8060 - val_MAPE: 10.8060\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3716 - MAPE: 6.3716 - val_loss: 11.0210 - val_MAPE: 11.0210\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3992 - MAPE: 6.3992 - val_loss: 10.8630 - val_MAPE: 10.8630\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7003 - MAPE: 6.7003 - val_loss: 10.9567 - val_MAPE: 10.9567\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4244 - MAPE: 6.4244 - val_loss: 11.0207 - val_MAPE: 11.0207\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4529 - MAPE: 6.4529 - val_loss: 11.1441 - val_MAPE: 11.1441\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3345 - MAPE: 6.3345 - val_loss: 11.1850 - val_MAPE: 11.1850\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4488 - MAPE: 6.4488 - val_loss: 10.7261 - val_MAPE: 10.7261\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5778 - MAPE: 6.5778 - val_loss: 11.3937 - val_MAPE: 11.3937\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5632 - MAPE: 6.5632 - val_loss: 10.7647 - val_MAPE: 10.7647\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5509 - MAPE: 6.5509 - val_loss: 10.6638 - val_MAPE: 10.6638\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4395 - MAPE: 6.4395 - val_loss: 10.8062 - val_MAPE: 10.8062\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3431 - MAPE: 6.3431 - val_loss: 10.8925 - val_MAPE: 10.8925\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4279 - MAPE: 6.4279 - val_loss: 10.8711 - val_MAPE: 10.8711\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5552 - MAPE: 6.5552 - val_loss: 10.7276 - val_MAPE: 10.7276\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3528 - MAPE: 6.3528 - val_loss: 10.6033 - val_MAPE: 10.6033\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5579 - MAPE: 6.5579 - val_loss: 11.0387 - val_MAPE: 11.0387\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6417 - MAPE: 6.6417 - val_loss: 11.1911 - val_MAPE: 11.1911\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4247 - MAPE: 6.4247 - val_loss: 11.2333 - val_MAPE: 11.2333\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4415 - MAPE: 6.4415 - val_loss: 10.9651 - val_MAPE: 10.9651\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3414 - MAPE: 6.3414 - val_loss: 10.6554 - val_MAPE: 10.6554\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5901 - MAPE: 6.5901 - val_loss: 10.8862 - val_MAPE: 10.8862\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5398 - MAPE: 6.5398 - val_loss: 11.2314 - val_MAPE: 11.2314\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2814 - MAPE: 6.2814 - val_loss: 10.9203 - val_MAPE: 10.9203\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0726 - MAPE: 6.0726 - val_loss: 10.7277 - val_MAPE: 10.7277\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2385 - MAPE: 6.2385 - val_loss: 10.8991 - val_MAPE: 10.8991\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3057 - MAPE: 6.3057 - val_loss: 11.1143 - val_MAPE: 11.1143\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4691 - MAPE: 6.4691 - val_loss: 11.3487 - val_MAPE: 11.3487\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6728 - MAPE: 6.6728 - val_loss: 10.4786 - val_MAPE: 10.4786\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2726 - MAPE: 6.2726 - val_loss: 10.7211 - val_MAPE: 10.7211\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2884 - MAPE: 6.2884 - val_loss: 10.6859 - val_MAPE: 10.6859\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2527 - MAPE: 6.2527 - val_loss: 11.3385 - val_MAPE: 11.3385\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5553 - MAPE: 6.5553 - val_loss: 11.4137 - val_MAPE: 11.4137\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3090 - MAPE: 6.3090 - val_loss: 10.6674 - val_MAPE: 10.6674\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2194 - MAPE: 6.2194 - val_loss: 10.6928 - val_MAPE: 10.6928\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1453 - MAPE: 6.1453 - val_loss: 10.6440 - val_MAPE: 10.6440\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0189 - MAPE: 6.0189 - val_loss: 10.9181 - val_MAPE: 10.9181\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1464 - MAPE: 6.1464 - val_loss: 10.9569 - val_MAPE: 10.9569\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2197 - MAPE: 6.2197 - val_loss: 10.9433 - val_MAPE: 10.9433\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9948 - MAPE: 5.9948 - val_loss: 10.8597 - val_MAPE: 10.8597\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9384 - MAPE: 5.9384 - val_loss: 10.7919 - val_MAPE: 10.7919\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3004 - MAPE: 6.3004 - val_loss: 10.5010 - val_MAPE: 10.5010\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0722 - MAPE: 6.0722 - val_loss: 10.8564 - val_MAPE: 10.8564\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9994 - MAPE: 5.9994 - val_loss: 10.7239 - val_MAPE: 10.7239\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9803 - MAPE: 5.9803 - val_loss: 10.5903 - val_MAPE: 10.5903\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2798 - MAPE: 6.2798 - val_loss: 10.9622 - val_MAPE: 10.9622\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2324 - MAPE: 6.2324 - val_loss: 10.8633 - val_MAPE: 10.8633\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0410 - MAPE: 6.0410 - val_loss: 10.9176 - val_MAPE: 10.9176\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2547 - MAPE: 6.2547 - val_loss: 11.1424 - val_MAPE: 11.1424\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5357 - MAPE: 6.5357 - val_loss: 10.6515 - val_MAPE: 10.6515\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9935 - MAPE: 5.9935 - val_loss: 10.5620 - val_MAPE: 10.5620\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0773 - MAPE: 6.0773 - val_loss: 10.6642 - val_MAPE: 10.6642\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2278 - MAPE: 6.2278 - val_loss: 10.4247 - val_MAPE: 10.4247\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9447 - MAPE: 5.9447 - val_loss: 10.4231 - val_MAPE: 10.4231\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7520 - MAPE: 5.7520 - val_loss: 10.5595 - val_MAPE: 10.5595\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7674 - MAPE: 5.7674 - val_loss: 10.5593 - val_MAPE: 10.5593\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9372 - MAPE: 5.9372 - val_loss: 10.6385 - val_MAPE: 10.6385\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8612 - MAPE: 5.8612 - val_loss: 10.7072 - val_MAPE: 10.7072\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7417 - MAPE: 5.7417 - val_loss: 10.7147 - val_MAPE: 10.7147\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7332 - MAPE: 5.7332 - val_loss: 10.3895 - val_MAPE: 10.3895\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8443 - MAPE: 5.8443 - val_loss: 10.6243 - val_MAPE: 10.6243\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7481 - MAPE: 5.7481 - val_loss: 10.6702 - val_MAPE: 10.6702\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8368 - MAPE: 5.8368 - val_loss: 10.9581 - val_MAPE: 10.9581\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9956 - MAPE: 5.9956 - val_loss: 11.0460 - val_MAPE: 11.0460\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8313 - MAPE: 5.8313 - val_loss: 10.2542 - val_MAPE: 10.2542\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1215 - MAPE: 6.1215 - val_loss: 10.3267 - val_MAPE: 10.3267\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4469 - MAPE: 6.4469 - val_loss: 11.0005 - val_MAPE: 11.0005\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1199 - MAPE: 6.1199 - val_loss: 10.7398 - val_MAPE: 10.7398\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8542 - MAPE: 5.8542 - val_loss: 10.5880 - val_MAPE: 10.5880\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1324 - MAPE: 6.1324 - val_loss: 10.2332 - val_MAPE: 10.2332\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9143 - MAPE: 5.9143 - val_loss: 10.3975 - val_MAPE: 10.3975\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7910 - MAPE: 5.7910 - val_loss: 10.5655 - val_MAPE: 10.5655\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8467 - MAPE: 5.8467 - val_loss: 10.8179 - val_MAPE: 10.8179\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0056 - MAPE: 6.0056 - val_loss: 10.6290 - val_MAPE: 10.6290\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8398 - MAPE: 5.8398 - val_loss: 10.3695 - val_MAPE: 10.3695\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6608 - MAPE: 5.6608 - val_loss: 10.4242 - val_MAPE: 10.4242\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6249 - MAPE: 5.6249 - val_loss: 10.6862 - val_MAPE: 10.6862\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5685 - MAPE: 5.5685 - val_loss: 10.3315 - val_MAPE: 10.3315\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9814 - MAPE: 5.9814 - val_loss: 10.3741 - val_MAPE: 10.3741\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8713 - MAPE: 5.8713 - val_loss: 10.6606 - val_MAPE: 10.6606\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5899 - MAPE: 5.5899 - val_loss: 10.2998 - val_MAPE: 10.2998\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5244 - MAPE: 5.5244 - val_loss: 10.6872 - val_MAPE: 10.6872\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7161 - MAPE: 5.7161 - val_loss: 10.6509 - val_MAPE: 10.6509\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9078 - MAPE: 5.9078 - val_loss: 10.2376 - val_MAPE: 10.2376\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8685 - MAPE: 5.8685 - val_loss: 10.2097 - val_MAPE: 10.2097\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6309 - MAPE: 5.6309 - val_loss: 10.3820 - val_MAPE: 10.3820\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6912 - MAPE: 5.6912 - val_loss: 10.6603 - val_MAPE: 10.6603\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7790 - MAPE: 5.7790 - val_loss: 10.3848 - val_MAPE: 10.3848\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7611 - MAPE: 5.7611 - val_loss: 10.4981 - val_MAPE: 10.4981\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5716 - MAPE: 5.5716 - val_loss: 11.0173 - val_MAPE: 11.0173\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8604 - MAPE: 5.8604 - val_loss: 10.6019 - val_MAPE: 10.6019\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4079 - MAPE: 5.4079 - val_loss: 10.2714 - val_MAPE: 10.2714\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5385 - MAPE: 5.5385 - val_loss: 10.5148 - val_MAPE: 10.5148\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4368 - MAPE: 5.4368 - val_loss: 10.4955 - val_MAPE: 10.4955\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5435 - MAPE: 5.5435 - val_loss: 10.4448 - val_MAPE: 10.4448\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3990 - MAPE: 5.3990 - val_loss: 10.2520 - val_MAPE: 10.2520\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3666 - MAPE: 5.3666 - val_loss: 10.2854 - val_MAPE: 10.2854\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5751 - MAPE: 5.5751 - val_loss: 10.3623 - val_MAPE: 10.3623\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2970 - MAPE: 5.2970 - val_loss: 10.4711 - val_MAPE: 10.4711\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4953 - MAPE: 5.4953 - val_loss: 10.2697 - val_MAPE: 10.2697\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4451 - MAPE: 5.4451 - val_loss: 10.5074 - val_MAPE: 10.5074\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5742 - MAPE: 5.5742 - val_loss: 10.8175 - val_MAPE: 10.8175\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5338 - MAPE: 5.5338 - val_loss: 10.4082 - val_MAPE: 10.4082\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4122 - MAPE: 5.4122 - val_loss: 9.9513 - val_MAPE: 9.9513\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3864 - MAPE: 5.3864 - val_loss: 10.1398 - val_MAPE: 10.1398\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4868 - MAPE: 5.4868 - val_loss: 10.5046 - val_MAPE: 10.5046\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4898 - MAPE: 5.4898 - val_loss: 10.4930 - val_MAPE: 10.4930\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2680 - MAPE: 5.2680 - val_loss: 10.1116 - val_MAPE: 10.1116\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4485 - MAPE: 5.4485 - val_loss: 10.1222 - val_MAPE: 10.1222\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6517 - MAPE: 5.6517 - val_loss: 10.1627 - val_MAPE: 10.1627\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4168 - MAPE: 5.4168 - val_loss: 10.3153 - val_MAPE: 10.3153\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3969 - MAPE: 5.3969 - val_loss: 10.0651 - val_MAPE: 10.0651\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5079 - MAPE: 5.5079 - val_loss: 10.3636 - val_MAPE: 10.3636\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6737 - MAPE: 5.6737 - val_loss: 10.3409 - val_MAPE: 10.3409\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7477 - MAPE: 5.7477 - val_loss: 10.3063 - val_MAPE: 10.3063\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5736 - MAPE: 5.5736 - val_loss: 10.1174 - val_MAPE: 10.1174\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5055 - MAPE: 5.5055 - val_loss: 10.1945 - val_MAPE: 10.1945\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5871 - MAPE: 5.5871 - val_loss: 10.0581 - val_MAPE: 10.0581\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5466 - MAPE: 5.5466 - val_loss: 10.0680 - val_MAPE: 10.0680\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3232 - MAPE: 5.3232 - val_loss: 10.2394 - val_MAPE: 10.2394\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5125 - MAPE: 5.5125 - val_loss: 10.1906 - val_MAPE: 10.1906\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6385 - MAPE: 5.6385 - val_loss: 10.0711 - val_MAPE: 10.0711\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5460 - MAPE: 5.5460 - val_loss: 10.2151 - val_MAPE: 10.2151\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4945 - MAPE: 5.4945 - val_loss: 10.7893 - val_MAPE: 10.7893\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3379 - MAPE: 5.3379 - val_loss: 10.3426 - val_MAPE: 10.3426\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1827 - MAPE: 5.1827 - val_loss: 10.5290 - val_MAPE: 10.5290\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f832ed4bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691c2f39-2005-4411-e69b-4d6cce849994"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952ce25d-8f77-4f3d-d790-7b7f67056da2"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 16s 5ms/step - loss: 437.4922 - accuracy: 0.0162 - val_loss: 437.4907 - val_accuracy: 0.0011\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0175 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0202 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0145 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4923 - accuracy: 0.0177 - val_loss: 437.4907 - val_accuracy: 0.0499\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4926 - accuracy: 0.0181 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4917 - accuracy: 0.0126 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0140 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0178 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0156 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0159 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4925 - accuracy: 0.0130 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0087 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0167 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0121 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0127 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0104 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0113 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0121 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0113 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0100 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0083 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0092 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4925 - accuracy: 0.0095 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4918 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0101 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4927 - accuracy: 0.0118 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0090 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0112 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0087 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0084 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0116 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0104 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4925 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83186ba210>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69673bd-3016-4cc1-df49-1c14ca4931c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c71669-69e1-45e3-9da0-c01c13a55b82"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0444 - accuracy: 0.1337 - val_loss: 0.0349 - val_accuracy: 0.2095\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0335 - accuracy: 0.2524 - val_loss: 0.0322 - val_accuracy: 0.2797\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0316 - accuracy: 0.3015 - val_loss: 0.0313 - val_accuracy: 0.3070\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0305 - accuracy: 0.3318 - val_loss: 0.0301 - val_accuracy: 0.3384\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.3548 - val_loss: 0.0294 - val_accuracy: 0.3586\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0290 - accuracy: 0.3726 - val_loss: 0.0290 - val_accuracy: 0.3742\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0284 - accuracy: 0.3877 - val_loss: 0.0285 - val_accuracy: 0.3816\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0278 - accuracy: 0.4005 - val_loss: 0.0279 - val_accuracy: 0.4031\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0273 - accuracy: 0.4157 - val_loss: 0.0276 - val_accuracy: 0.4108\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0268 - accuracy: 0.4264 - val_loss: 0.0277 - val_accuracy: 0.4043\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0265 - accuracy: 0.4343 - val_loss: 0.0272 - val_accuracy: 0.4206\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0261 - accuracy: 0.4417 - val_loss: 0.0270 - val_accuracy: 0.4286\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0258 - accuracy: 0.4498 - val_loss: 0.0269 - val_accuracy: 0.4242\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0256 - accuracy: 0.4571 - val_loss: 0.0267 - val_accuracy: 0.4321\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0253 - accuracy: 0.4631 - val_loss: 0.0265 - val_accuracy: 0.4392\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0251 - accuracy: 0.4660 - val_loss: 0.0265 - val_accuracy: 0.4402\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0249 - accuracy: 0.4714 - val_loss: 0.0264 - val_accuracy: 0.4420\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0246 - accuracy: 0.4774 - val_loss: 0.0261 - val_accuracy: 0.4499\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0245 - accuracy: 0.4810 - val_loss: 0.0264 - val_accuracy: 0.4419\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0243 - accuracy: 0.4860 - val_loss: 0.0259 - val_accuracy: 0.4537\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0241 - accuracy: 0.4910 - val_loss: 0.0257 - val_accuracy: 0.4583\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0240 - accuracy: 0.4942 - val_loss: 0.0257 - val_accuracy: 0.4579\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0238 - accuracy: 0.4989 - val_loss: 0.0262 - val_accuracy: 0.4537\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0237 - accuracy: 0.4995 - val_loss: 0.0258 - val_accuracy: 0.4557\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0236 - accuracy: 0.5028 - val_loss: 0.0260 - val_accuracy: 0.4551\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0234 - accuracy: 0.5063 - val_loss: 0.0257 - val_accuracy: 0.4622\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0233 - accuracy: 0.5111 - val_loss: 0.0261 - val_accuracy: 0.4551\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0232 - accuracy: 0.5136 - val_loss: 0.0261 - val_accuracy: 0.4482\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0231 - accuracy: 0.5152 - val_loss: 0.0257 - val_accuracy: 0.4651\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0230 - accuracy: 0.5174 - val_loss: 0.0255 - val_accuracy: 0.4677\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0229 - accuracy: 0.5193 - val_loss: 0.0259 - val_accuracy: 0.4652\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0228 - accuracy: 0.5233 - val_loss: 0.0256 - val_accuracy: 0.4690\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0227 - accuracy: 0.5259 - val_loss: 0.0255 - val_accuracy: 0.4704\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0226 - accuracy: 0.5247 - val_loss: 0.0254 - val_accuracy: 0.4663\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0225 - accuracy: 0.5305 - val_loss: 0.0254 - val_accuracy: 0.4725\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0225 - accuracy: 0.5301 - val_loss: 0.0259 - val_accuracy: 0.4625\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0223 - accuracy: 0.5334 - val_loss: 0.0255 - val_accuracy: 0.4720\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0223 - accuracy: 0.5349 - val_loss: 0.0255 - val_accuracy: 0.4694\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0222 - accuracy: 0.5355 - val_loss: 0.0255 - val_accuracy: 0.4655\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0221 - accuracy: 0.5372 - val_loss: 0.0256 - val_accuracy: 0.4671\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0220 - accuracy: 0.5393 - val_loss: 0.0255 - val_accuracy: 0.4689\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0220 - accuracy: 0.5381 - val_loss: 0.0254 - val_accuracy: 0.4746\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0219 - accuracy: 0.5396 - val_loss: 0.0256 - val_accuracy: 0.4689\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0219 - accuracy: 0.5418 - val_loss: 0.0258 - val_accuracy: 0.4661\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0218 - accuracy: 0.5459 - val_loss: 0.0261 - val_accuracy: 0.4661\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0217 - accuracy: 0.5466 - val_loss: 0.0258 - val_accuracy: 0.4721\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0216 - accuracy: 0.5506 - val_loss: 0.0255 - val_accuracy: 0.4730\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0216 - accuracy: 0.5508 - val_loss: 0.0257 - val_accuracy: 0.4733\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0215 - accuracy: 0.5528 - val_loss: 0.0258 - val_accuracy: 0.4742\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0215 - accuracy: 0.5534 - val_loss: 0.0255 - val_accuracy: 0.4684\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f831a011a90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe.\n",
        "\n",
        "## Answer 7\n",
        "The code tries to use the binary_crossentropy loss function, which works with 2 classes and 1 output node (the output node outputs 0 for 1 class and 1 for the other), while this problem has 20 classes and multiple output nodes. In addition, some dropout or batch normalization layers can be added to improve performance."
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c836608c-0b18-4b94-99f6-e0f3047cb683"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5827 - accuracy: 0.2039 - val_loss: 2.3462 - val_accuracy: 0.2708\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.2226 - accuracy: 0.3103 - val_loss: 2.1945 - val_accuracy: 0.3225\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0795 - accuracy: 0.3513 - val_loss: 2.0654 - val_accuracy: 0.3511\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9812 - accuracy: 0.3812 - val_loss: 1.9735 - val_accuracy: 0.3827\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.9014 - accuracy: 0.4037 - val_loss: 1.9024 - val_accuracy: 0.4054\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8421 - accuracy: 0.4218 - val_loss: 1.9414 - val_accuracy: 0.3934\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7985 - accuracy: 0.4376 - val_loss: 1.8808 - val_accuracy: 0.4105\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7582 - accuracy: 0.4473 - val_loss: 1.8534 - val_accuracy: 0.4238\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7260 - accuracy: 0.4578 - val_loss: 1.8808 - val_accuracy: 0.4135\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6979 - accuracy: 0.4641 - val_loss: 1.8473 - val_accuracy: 0.4291\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6699 - accuracy: 0.4723 - val_loss: 1.9015 - val_accuracy: 0.4159\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6465 - accuracy: 0.4784 - val_loss: 1.7957 - val_accuracy: 0.4394\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6267 - accuracy: 0.4853 - val_loss: 1.8219 - val_accuracy: 0.4326\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6056 - accuracy: 0.4922 - val_loss: 1.7826 - val_accuracy: 0.4465\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5826 - accuracy: 0.5004 - val_loss: 1.7418 - val_accuracy: 0.4581\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5671 - accuracy: 0.5043 - val_loss: 1.7705 - val_accuracy: 0.4525\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5516 - accuracy: 0.5083 - val_loss: 1.8020 - val_accuracy: 0.4489\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5307 - accuracy: 0.5155 - val_loss: 1.7771 - val_accuracy: 0.4532\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5167 - accuracy: 0.5195 - val_loss: 1.8032 - val_accuracy: 0.4452\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5038 - accuracy: 0.5229 - val_loss: 1.7825 - val_accuracy: 0.4539\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4833 - accuracy: 0.5276 - val_loss: 1.7873 - val_accuracy: 0.4551\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4726 - accuracy: 0.5329 - val_loss: 1.7431 - val_accuracy: 0.4642\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4613 - accuracy: 0.5338 - val_loss: 1.7809 - val_accuracy: 0.4512\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4498 - accuracy: 0.5385 - val_loss: 1.7664 - val_accuracy: 0.4593\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4406 - accuracy: 0.5396 - val_loss: 1.7722 - val_accuracy: 0.4599\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83196673d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    \n",
        "    loss = np.sum((true*np.log(pred)))*(-1/len(pred))\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f64103-d2c3-4480-9e02-e6b178562ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3445815443992615\n",
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbb2b8a-7d47-4cfd-ef81-aa712b58ff1e"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5929 - accuracy: 0.2008 - val_loss: 2.3916 - val_accuracy: 0.2519\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2368 - accuracy: 0.3092 - val_loss: 2.1513 - val_accuracy: 0.3304\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0869 - accuracy: 0.3531 - val_loss: 2.0403 - val_accuracy: 0.3622\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9927 - accuracy: 0.3829 - val_loss: 2.0016 - val_accuracy: 0.3758\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9214 - accuracy: 0.4027 - val_loss: 1.9560 - val_accuracy: 0.3892\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8574 - accuracy: 0.4201 - val_loss: 1.9058 - val_accuracy: 0.4044\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8016 - accuracy: 0.4373 - val_loss: 1.8775 - val_accuracy: 0.4170\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7582 - accuracy: 0.4489 - val_loss: 1.8504 - val_accuracy: 0.4245\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7225 - accuracy: 0.4588 - val_loss: 1.8305 - val_accuracy: 0.4329\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6873 - accuracy: 0.4695 - val_loss: 1.7985 - val_accuracy: 0.4363\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6551 - accuracy: 0.4802 - val_loss: 1.8013 - val_accuracy: 0.4391\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6271 - accuracy: 0.4890 - val_loss: 1.7940 - val_accuracy: 0.4429\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6045 - accuracy: 0.4931 - val_loss: 1.7712 - val_accuracy: 0.4519\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5800 - accuracy: 0.5037 - val_loss: 1.7820 - val_accuracy: 0.4521\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5579 - accuracy: 0.5085 - val_loss: 1.7676 - val_accuracy: 0.4522\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5416 - accuracy: 0.5157 - val_loss: 1.7561 - val_accuracy: 0.4605\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5197 - accuracy: 0.5212 - val_loss: 1.7470 - val_accuracy: 0.4627\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5043 - accuracy: 0.5230 - val_loss: 1.7478 - val_accuracy: 0.4595\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4903 - accuracy: 0.5278 - val_loss: 1.7749 - val_accuracy: 0.4540\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4731 - accuracy: 0.5320 - val_loss: 1.7691 - val_accuracy: 0.4541\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4600 - accuracy: 0.5376 - val_loss: 1.7535 - val_accuracy: 0.4586\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4430 - accuracy: 0.5415 - val_loss: 1.8184 - val_accuracy: 0.4524\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4311 - accuracy: 0.5468 - val_loss: 1.7898 - val_accuracy: 0.4576\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4251 - accuracy: 0.5468 - val_loss: 1.7628 - val_accuracy: 0.4648\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4125 - accuracy: 0.5532 - val_loss: 1.7856 - val_accuracy: 0.4664\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4020 - accuracy: 0.5546 - val_loss: 1.7711 - val_accuracy: 0.4606\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3898 - accuracy: 0.5558 - val_loss: 1.7907 - val_accuracy: 0.4637\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3776 - accuracy: 0.5602 - val_loss: 1.7827 - val_accuracy: 0.4632\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3667 - accuracy: 0.5652 - val_loss: 1.8054 - val_accuracy: 0.4580\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3572 - accuracy: 0.5664 - val_loss: 1.7728 - val_accuracy: 0.4606\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3487 - accuracy: 0.5700 - val_loss: 1.7832 - val_accuracy: 0.4629\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3400 - accuracy: 0.5717 - val_loss: 1.7877 - val_accuracy: 0.4616\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3327 - accuracy: 0.5733 - val_loss: 1.7955 - val_accuracy: 0.4638\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3286 - accuracy: 0.5768 - val_loss: 1.8324 - val_accuracy: 0.4549\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3193 - accuracy: 0.5785 - val_loss: 1.7995 - val_accuracy: 0.4675\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3070 - accuracy: 0.5809 - val_loss: 1.8039 - val_accuracy: 0.4677\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3061 - accuracy: 0.5844 - val_loss: 1.8163 - val_accuracy: 0.4619\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2980 - accuracy: 0.5851 - val_loss: 1.8231 - val_accuracy: 0.4605\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2893 - accuracy: 0.5874 - val_loss: 1.8371 - val_accuracy: 0.4609\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2813 - accuracy: 0.5885 - val_loss: 1.8203 - val_accuracy: 0.4640\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2760 - accuracy: 0.5912 - val_loss: 1.8337 - val_accuracy: 0.4631\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2699 - accuracy: 0.5918 - val_loss: 1.8866 - val_accuracy: 0.4555\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2664 - accuracy: 0.5921 - val_loss: 1.8611 - val_accuracy: 0.4543\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2608 - accuracy: 0.5943 - val_loss: 1.8755 - val_accuracy: 0.4585\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2509 - accuracy: 0.5985 - val_loss: 1.8662 - val_accuracy: 0.4568\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2467 - accuracy: 0.5997 - val_loss: 1.8911 - val_accuracy: 0.4610\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2484 - accuracy: 0.5990 - val_loss: 1.8761 - val_accuracy: 0.4643\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2395 - accuracy: 0.6007 - val_loss: 1.8595 - val_accuracy: 0.4667\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2383 - accuracy: 0.6004 - val_loss: 1.8999 - val_accuracy: 0.4605\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2284 - accuracy: 0.6038 - val_loss: 1.8715 - val_accuracy: 0.4659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83191a1390>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "\n",
        "## Answer 9\n",
        "A Multi-class classification problem involves classes which are mutually exclusive. As a result, sparce categorical cross entropy is best suited for this type of problem, but it can also be done using categorical cross entropy with one-hot vectors. In contrast, Multi-label classification problems contain classes that are not mutually exclusive. Multi-label tasks can attribute multiple classes due to having multiple labels. So, categorical cross entropy must be used for this type of problem (with multi-hot arrays), since the inputs allow for multiple identifications within there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "## Answer 10\n",
        "Binary cross entropy and categorical cross entropy both use the cross entropy function, but binary cross entropy is specialized for binary classification (2 mutualy exclusive classes, with output determined by a single node), while categorical cross entropy is used for muli-class classification (2+ classes, multiple output nodes, may or may not be mutually exclusive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "## Answer 11\n",
        "Both Sparse Cross entropy and Categorical Cross entropy use the same loss function behind the scenes (categorical loss), and differ only in the way they take inputs (sparse takes integer values, categrorical takes arrays), where sparse is more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}